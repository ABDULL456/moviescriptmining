{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generate_samples.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "oPfIIzeAKjan",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q gpt_2_simple\n",
        "!mkdir /content/results\n",
        "!mkdir /content/fake\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "import json\n",
        "import numpy as np\n",
        "import random\n",
        "from os import listdir\n",
        "from os.path import isfile, join, splitext\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "import nltk.sentiment.vader as vader\n",
        "import shutil"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Dq-cnJ265N_i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nltk.download('vader_lexicon')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lwY5VOHJ51oo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gpt2.download_gpt2()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ho8HjzNgKlWg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gpt2.mount_gdrive()\n",
        "gpt2.copy_checkpoint_from_gdrive()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zhx9E_jKIE0P",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sid = vader.SentimentIntensityAnalyzer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xHTQkI8vQbvB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample_random(sess, prefix, temp=0.7, top_k=0):\n",
        "  return \"\".join(gpt2.generate(sess, \n",
        "                               return_as_list=True,\n",
        "                               length=100, \n",
        "                               temperature=temp, \n",
        "                               prefix=prefix, \n",
        "                               top_k=top_k, \n",
        "                               include_prefix=False)\n",
        "                )[len(prefix):]\n",
        "\n",
        "def sample_sentiment(sess, prefix, sentiment_vec, max_iter=15, thresh=0):\n",
        "  \"\"\" \n",
        "  Sentiment based rejection sampling\n",
        "  returns the best sample\n",
        "  Sample either hundred times or when we do better than threshold distance\n",
        "  note thresh is between [0,2]. It's an early cutoff\n",
        "  \"\"\"\n",
        "  best_score = 2\n",
        "  best_result = None\n",
        "  for i in range(max_iter):\n",
        "    sample = sample_random(sess, prefix, random.random())\n",
        "    if not sample:\n",
        "      continue\n",
        "    ss = sid.polarity_scores(sample)\n",
        "    curr_sent = np.array([ss[\"compound\"], ss[\"neg\"], ss[\"neu\"], ss[\"pos\"]])\n",
        "    dist = np.linalg.norm(sentiment_vec-curr_sent)\n",
        "    if (dist < thresh):\n",
        "      return sample\n",
        "    if dist < best_score:\n",
        "      best_result = sample\n",
        "      best_score = dist\n",
        "  return best_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GMPXemZtNTI4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def test_single(sid, train_file, test_instance, save_dest):\n",
        "  \"\"\"\n",
        "  Takes the single testing JSON file as a dict.\n",
        "  Runs sampling on all 3, saves results to dest dir, each training example to\n",
        "  its own file\n",
        "  \"\"\"\n",
        "  tf.reset_default_graph()\n",
        "  sess = gpt2.start_tf_sess()\n",
        "#   shutil.copyfile('drive/My Drive/checkpoint/run1/checkpoint', 'checkpoint/run1/checkpoint')\n",
        "  \n",
        "  gpt2.finetune(sess,\n",
        "                dataset=train_file,\n",
        "                steps=2,\n",
        "                print_every=5,\n",
        "                sample_every=1000,\n",
        "                sample_length=1,\n",
        "                restore_from='fresh',\n",
        "                save_every=30,\n",
        "                run_name=\"fake/\"\n",
        "                ) # don't want to save\n",
        "  print('finetune done')\n",
        "  results = []\n",
        "  for i in range(1,4):\n",
        "    inp = test_instance[\"test{}_input\".format(i)]\n",
        "    tru = test_instance[\"test{}_true\".format(i)]\n",
        "    id_ = test_instance[\"test{}_id\".format(i)]\n",
        "    inp = \"\".join(inp.split(\"\\n\")[-4:])\n",
        "    # Generate random sample\n",
        "    samp_rand = sample_random(sess, inp)\n",
        "    # Generate sentiment sample\n",
        "    ss = sid.polarity_scores(tru)\n",
        "    sentiment_vec = np.array([ss[\"compound\"], ss[\"neg\"], ss[\"neu\"], ss[\"pos\"]])\n",
        "    samp_sent = sample_sentiment(sess, inp, sentiment_vec)\n",
        "    sample = {\"id\": id_,\n",
        "              \"prefix\": inp,\n",
        "              \"truth\": tru,\n",
        "              \"random\": samp_rand,\n",
        "              \"sentiment\": samp_sent\n",
        "             }\n",
        "    print(\"Sample: {}\".format(sample))\n",
        "    results.append(sample)\n",
        "  with open(save_dest, \"w\") as f:\n",
        "    json.dump(results, f)\n",
        "  print('finished test single')\n",
        "\n",
        "def run_tests(test_dir, train_dir):\n",
        "  # Load model from checkpoint\n",
        "  files = sorted(listdir(train_dir))[3::4] #change to your number\n",
        "  for f in files:\n",
        "    fname = splitext(f)[0]\n",
        "    train = join(train_dir, f)\n",
        "    test = join(test_dir, fname+\".json\")\n",
        "    if isfile(train) and isfile(test):\n",
        "      with open(test, \"r\") as test_file:\n",
        "        test_instance = json.load(test_file)\n",
        "        print(f'Testing {fname}')\n",
        "        test_single(sid, train, test_instance, \"/content/results/{}_sample.json\".format(f))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tG682gbVNp_C",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "run_tests('/content/drive/My Drive/test_cases/', '/content/drive/My Drive/training_cases/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sgXWCFd1hmMO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}