{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_fb8s1jbkTBG"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QzYrJQVLFr5c"
   },
   "outputs": [],
   "source": [
    "class Sample:\n",
    "    help_text = '\\n'.join([\n",
    "        'Type A, B, and C in order of preference.',\n",
    "        'For example, if you think B is the best and A is the worst,',\n",
    "        'type \"bac or BAC\".',\n",
    "    ])\n",
    "  \n",
    "    def __init__(self, prompt, truth, random, sentiment, rank=None):\n",
    "        self.prompt = prompt\n",
    "        self.truth = truth\n",
    "        self.random = random\n",
    "        self.sentiment = sentiment\n",
    "        self.rank = rank\n",
    "    \n",
    "    @staticmethod\n",
    "    def load_file(path):\n",
    "        '''\n",
    "        Create a list of Samples from a json file.\n",
    "        The json should be either a single dict or a list of dicts.\n",
    "        Each dict must contain the keys \"prompt\", \"truth\", \"random\", and \"sentiment\".\n",
    "        '''\n",
    "        with open(path) as f:\n",
    "            j = json.load(f)\n",
    "        if isinstance(j, dict):\n",
    "            j = [j]\n",
    "        return [Sample.load_dict(d) for d in j]\n",
    "  \n",
    "    @staticmethod\n",
    "    def load_dict(d):\n",
    "        return Sample(d['prompt'], d['truth'], d['random'], d['sentiment'], d.get('rank'))\n",
    "  \n",
    "    def evaluate(self):\n",
    "        def print_header(text=None, c='='):\n",
    "            if not text:\n",
    "                print(c * 62)\n",
    "                return\n",
    "            left = max(30 - len(text) // 2, 10)\n",
    "            right = max(30 - (len(text) + 1) // 2, 10)\n",
    "            print(c * left, text, c * right)\n",
    "  \n",
    "        print_header('Prompt')\n",
    "        print(self.prompt)\n",
    "        completions = [self.truth, self.random, self.sentiment]\n",
    "        indices = list('ABC')\n",
    "        random.shuffle(indices)\n",
    "        mapping = {i: ci for i, ci in zip(indices, range(3))}\n",
    "        for i in 'ABC':\n",
    "            print_header(f'Completion {i}', c='-')\n",
    "            print(completions[mapping[i]])\n",
    "    \n",
    "        while True:\n",
    "            print_header(c='-')\n",
    "            res = input(f'Rank samples ([h]elp, [q]uit) >>> ')\n",
    "            if res.startswith('h'):\n",
    "                print(self.help_text)\n",
    "                continue\n",
    "            if res.startswith('q'):\n",
    "                return None\n",
    "            res = res.upper()\n",
    "            if len(res) != 3:\n",
    "                print('Invalid rank, try again.')\n",
    "            else:\n",
    "                res = [res[0],res[1],res[2]]\n",
    "                if set(res) != {'A', 'B', 'C'}:\n",
    "                    print('Invalid rank, try again.')\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "        rank = []\n",
    "        for i in res:\n",
    "            ci = mapping[i]\n",
    "            rank.append(['truth', 'random', 'sentiment'][ci])\n",
    "        self.rank = rank\n",
    "  \n",
    "    def __str__(self):\n",
    "        j = {\n",
    "            'prompt': self.prompt,\n",
    "            'truth': self.truth,\n",
    "            'random': self.random,\n",
    "            'sentiment': self.sentiment,\n",
    "        }\n",
    "        if self.rank:\n",
    "            j['rank'] = self.rank\n",
    "        return json.dumps(j, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 474
    },
    "colab_type": "code",
    "id": "04aHySLPIHDf",
    "outputId": "5b11cc94-c94c-4300-f63f-4b885215f73e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================== Prompt ===========================\n",
      "the prompt\n",
      "------------------------ Completion A ------------------------\n",
      "the sentiment\n",
      "------------------------ Completion B ------------------------\n",
      "the truth\n",
      "------------------------ Completion C ------------------------\n",
      "the random\n",
      "--------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Rank samples ([h]elp, [q]uit) >>>  abc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"prompt\": \"the prompt\",\n",
      "    \"truth\": \"the truth\",\n",
      "    \"random\": \"the random\",\n",
      "    \"sentiment\": \"the sentiment\",\n",
      "    \"rank\": [\n",
      "        \"sentiment\",\n",
      "        \"truth\",\n",
      "        \"random\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "s = Sample('the prompt', 'the truth', 'the random', 'the sentiment')\n",
    "s.evaluate()\n",
    "\n",
    "print(Sample.load_dict(json.loads(str(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "sample_evaluation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
