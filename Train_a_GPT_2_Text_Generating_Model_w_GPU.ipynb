{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train a GPT-2 Text-Generating Model w/ GPU",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "H7LoMj4GA4n_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#  Train a GPT-2 Text-Generating Model w/ GPU For Free \n",
        "\n",
        "by [Max Woolf](http://minimaxir.com)\n",
        "\n",
        "*Last updated: April 22, 2019*\n",
        "\n",
        "Retrain an advanced text generating neural network on any text dataset **for free on a GPU using Collaboratory** using `gpt-2-simple`!\n",
        "\n",
        "For more about `gpt-2-simple`, you can visit [this GitHub repository](https://github.com/minimaxir/gpt-2-simple).\n",
        "\n",
        "\n",
        "To get started:\n",
        "\n",
        "1. Copy this notebook to your Google Drive to keep it and save your changes. (File -> Save a Copy in Drive)\n",
        "2. Make sure you're running the notebook in Google Chrome.\n",
        "3. Run the cells below:\n"
      ]
    },
    {
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -q gpt_2_simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0wXB05bPDYxS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Downloading GPT-2\n",
        "\n",
        "If you're retraining a model on new text, you need to download the GPT-2 model first. The next cell downloads it from Google Cloud Storage and saves it in the Colaboratory at `/models/117M`.\n",
        "\n",
        "This model isn't permanently saved in the Colaboratory VM; you'll have to redownload it if you want to retrain it at a later time."
      ]
    },
    {
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab_type": "code",
        "outputId": "d08e7d8d-2e2d-4b4a-fcc2-fc7afe459d56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "cell_type": "code",
      "source": [
        "gpt2.download_gpt2()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.00kit [00:00, 756kit/s]                                                      \n",
            "Fetching encoder.json: 1.04Mit [00:00, 50.4Mit/s]                                                   \n",
            "Fetching hparams.json: 1.00kit [00:00, 1.11Mit/s]                                                   \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:07, 67.0Mit/s]                                  \n",
            "Fetching model.ckpt.index: 6.00kit [00:00, 1.77Mit/s]                                               \n",
            "Fetching model.ckpt.meta: 472kit [00:00, 31.8Mit/s]                                                 \n",
            "Fetching vocab.bpe: 457kit [00:00, 41.4Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "N8KXuKWzQSsN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Mounting Google Drive\n",
        "\n",
        "The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model *out* of Colaboratory, is to route it through Google Drive *first*.\n",
        "\n",
        "Running this cell (which will only work in Colaboratory) will mount your personal Google Drive in the VM, which later cells can use to get data in/out. (it will ask for an auth code; that auth is not saved anywhere)"
      ]
    },
    {
      "metadata": {
        "id": "puq4iC6vUAHc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "dec6ef54-a729-4b60-a3e2-43b745139d1a"
      },
      "cell_type": "code",
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BT__brhBCvJu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Uploading a Text File to be Trained to Colaboratory\n",
        "\n",
        "In the Colaboratory Notebook sidebar on the left of the screen, select *Files*. From there you can upload files:\n",
        "\n",
        "![alt text](https://i.imgur.com/TGcZT4h.png)\n",
        "\n",
        "Upload **any smaller text file**  (<10 MB) and update the file name in the cell below, then run the cell."
      ]
    },
    {
      "metadata": {
        "id": "6OFnPCLADfll",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file_name = \"12-Years-a-Slave.train.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HeeSKtNWUedE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If your text file is larger than 10MB, it is recommended to upload that file to Google Drive first, then copy that file from Google Drive to the Colaboratory VM."
      ]
    },
    {
      "metadata": {
        "id": "-Z6okFD8VKtS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gpt2.copy_file_from_gdrive(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LdpZQXknFNY3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Finetune GPT-2\n",
        "\n",
        "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
        "\n",
        "\n",
        "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "* **`sample_every`**: Number of steps to print example output\n",
        "* **`print_every`**: Number of steps to print training progress.\n",
        "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)"
      ]
    },
    {
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab_type": "code",
        "outputId": "cdc9770b-f9c0-4229-a8a0-d90d7f452c64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        }
      },
      "cell_type": "code",
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              steps=50,\n",
        "              restore_from='fresh',\n",
        "              print_every=10,\n",
        "              sample_every=200,\n",
        "              save_every=500\n",
        "              )"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:51: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:53: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.random.categorical instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "Loading checkpoint models/117M/model.ckpt\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from models/117M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 1/1 [00:00<00:00,  3.49it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 30360 tokens\n",
            "Training...\n",
            "[10 | 28.60] loss=2.92 avg=2.92\n",
            "[20 | 50.52] loss=2.53 avg=2.72\n",
            "[30 | 72.83] loss=2.09 avg=2.51\n",
            "[40 | 95.60] loss=1.65 avg=2.29\n",
            "[50 | 118.55] loss=1.63 avg=2.16\n",
            "Saving checkpoint/run1/model-50\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IXSuTNERaw6K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "After the model is trained, you can copy the checkpoint folder to your own Google Drive.\n",
        "\n",
        "If you want to download it to your personal computer, it's strongly recommended you copy it there first, then download from Google Drive."
      ]
    },
    {
      "metadata": {
        "id": "VHdTL8NDbAh3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gpt2.copy_checkpoint_to_gdrive()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qQJgV_b4bmzd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You're done! Feel free to go to the **Generate Text From The Trained Model** section to generate text based on your retrained model."
      ]
    },
    {
      "metadata": {
        "id": "pel-uBULXO2L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load a Trained Model Checkpoint\n",
        "\n",
        "Running the next cell will copy the `checkpoint` folder from your Google Drive into the Colaboratory VM."
      ]
    },
    {
      "metadata": {
        "id": "DCcx5u7sbPTD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gpt2.copy_checkpoint_from_gdrive()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RTa6zf3e_9gV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The next cell will allow you to load the retrained model checkpoint + metadata necessary to generate text.\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files."
      ]
    },
    {
      "metadata": {
        "id": "-fxL77nvAMAX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "gpt2.load_gpt2(sess)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ClJwpF_ACONp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate Text From The Trained Model\n",
        "\n",
        "After you've trained the model or loaded a retrained model from checkpoint, you can now generate text. `generate` generates a single text from the loaded model."
      ]
    },
    {
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "colab_type": "code",
        "outputId": "31da6e0c-f0d3-4ee7-91e6-67c5983341b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        }
      },
      "cell_type": "code",
      "source": [
        "gpt2.generate(sess)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'Tis the season, yah? It's the new year, yah? A new start comin' down at bay! The young slaves are off, returning for the winter break. They commence the ride south on the steamboat. A SLAVES RUNTS UP. \n",
            "INT. FREEMAN'S - LATER \n",
            "The slaves ride along the steamboat's deck, undulating along the brick path. The steamboat is slick with snow, and the slaves are in no mood for rough water. They are particularly drawn to the view of Lake Champlain. The water is slick and slick, and the slaves are drawn to it. \n",
            "INT. FREEMAN'S - LATER \n",
            "The slaves are riding on the steamboat's deck toward the sight of Lake Champlain. They are excited. And excited they are at the prospect of a new season. \n",
            "(CONTINUED) \n",
            "105 CONTINUED: 105 \n",
            "FREEMAN Move along. Move along. Move along. The slaves ride along, their excitement building. \n",
            "SOLOMON If anyone tries to ride along, they will be attacked. The slaves are attacked, and beaten until only one remains. \n",
            "FREEMAN (CONT'D) They are to be led to a cart loaded with timber. They are led in by wagon, and into the cart. The wagon carries them to Hamilton's, where the slaves are taken to a wagon-riding overseer. He lays down their property, and orders them to \"START WORKING WITH THEIR TISSUE.\" \n",
            "HIS WORK PERIOD \n",
            "105 CONTINUED: (2) 105 \n",
            "SOLOMON Yes, sir. Three weeks. \n",
            "OMITTED \n",
            "105 CONTINUED: (3) 105 Hamilton takes the cart up to the harbor's waterway. They are met by a swarm of slaves. They begin to work on Hamilton's wagon. The slaves are smart, hard working whites. Hamilton's crew is well paid, well dressed, and well dressed in black. \n",
            "INT. FREEMAN'S - LATER \n",
            "The slaves are working on Hamilton's wagon. They are punctual, diligent. They don't take much rest. \n",
            "EXT. FREEMAN'S - DUSK/DECK - DAY \n",
            "-A41- \n",
            "(CONTINUED) \n",
            "11:00 AM \n",
            "11:30 AM \n",
            "INT. FREEMAN'S - LATER \n",
            "Hamilton is now preparing to ride on the wagon. The slaves are all busy riding on the wagon's deck. A JUMBLE OF PAPER BEATING erupts from the vessel. The slaves are filled with panic. They are all made to stand. The overseer hovers about them. He is stern with a certain amount of anger. \n",
            "EXT. FREEMAN'S - DUSK/DECK - DAY \n",
            "-A42- \n",
            "13:00 AM \n",
            "13:00 AM \n",
            "INT. FREEMAN'S - LATER \n",
            "Hamilton is now out steaming with his new load. He arrives to a dock just beyond the city. Scattered about are the slaves, giving a hearty flutter as they are loaded into the ship. \n",
            "13:00 - \n",
            "INT. FREEMAN'S - DOCK - - DAY \n",
            "-A44- \n",
            "13:00 - \n",
            "13:00 \n",
            "INT. FREEMAN'S - DOCK - MAY/SEPTEMBER, 1841- \n",
            "Hamilton arrives to the dock. He sits down to eat. The slaves all drink. Solomon gives Hamilton a tip before rolling down the stairs. Hamilton follows suit. \n",
            "HAMILTON What's the matter with the Captain's tub?\" - \n",
            "FORD \n",
            "(CONTINUED) \n",
            "13:30 - \n",
            "13:30 \n",
            "INT. FREEMAN'S - DOCK - CONTINUOUS \n",
            "-MID MAY/SEPTEMBER, 1841- \n",
            "FORD and HAMILTON are now in the keeping of the \"Papyrus Cape,\" a large, white vessel of various colors. Built of timber, like a sack, it sits high above the water's surface. One end is decked out in stripes, the other in gold and red stripes. The other end is adorned by a cross with a SHAW holding it in both hands. A dog collar is tied around the neck of the dog's neck. The other end of the collar is fastened to a pole. A handcart is brought to the piazza by way of Captain Hamilton. \n",
            "HAMILTON You will fetch me back from Orleans. \n",
            "FORD The Negro is my property. My freedom is in my hands. \n",
            "HAMILTON Yes, sir. \n",
            "FORD And you\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oF4-PqF0Fl7R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If you're creating an API based on your model and need to pass the generated text elsewhere, you can do `text = gpt2.generate(sess, return_as_list=True)[0]`\n",
        "\n",
        "You can also pass in a `prefix` to the generate function to force the text to start with a given character sequence and generate text from there (good if you add an indicator when the text starts).\n",
        "\n",
        "You can also generate multiple texts at a time by specifing `nsamples`. Unique to GPT-2, you can pass a `batch_size` to generate multiple samples in parallel, giving a massive speedup (in Colaboratory, set a maximum of 20 for `batch_size`).\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.generate` and friends:\n",
        "\n",
        "*  **`length`**: Number of tokens to generate (default 1023, the maximum)\n",
        "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
        "* **`truncate`**: Truncates the input text until a given sequence, excluding that sequence (e.g. if `truncate='<|endoftext|>'`, the returned text will include everything before the first `<|endoftext|>`). It may be useful to combine this with a smaller `length` if the input texts are short.\n",
        "*  **`include_prefix`**: If using `truncate` and `include_prefix=False`, the specified `prefix` will not be included in the returned text."
      ]
    },
    {
      "metadata": {
        "id": "kL4JHhDfpAzI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "prefix = \"\"\"PATSEY All I ask: end my life. Take my body to the margin of the swamp-- Solomon looks at Patsey as though she were insane.\n",
        "SOLOMON No. \n",
        "PATSEY Take me by the throat. Hold me low in the water until I's still 'n without life. Bury me in a lonely place of dyin'. \n",
        "SOLOMON\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8DKMc0fiej4N",
        "colab_type": "code",
        "outputId": "c55ea54c-4f4e-46cc-d847-0879f5e4b5c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1691
        }
      },
      "cell_type": "code",
      "source": [
        "gpt2.generate(sess,\n",
        "              length=250,\n",
        "              temperature=0.7,\n",
        "              prefix=prefix,\n",
        "              nsamples=5,\n",
        "              batch_size=5\n",
        "              )"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PATSEY All I ask: end my life. Take my body to the margin of the swamp-- Solomon looks at Patsey as though she were insane.\n",
            "SOLOMON No. \n",
            "PATSEY Take me by the throat. Hold me low in the water until I's still 'n without life. Bury me in a lonely place of dyin'. \n",
            "SOLOMON No. \n",
            "(CONTINUED) \n",
            "35 CONTINUED: 35 \n",
            "PATSEY \n",
            "(MORE) \n",
            "35 \n",
            "SOLOMON No... \n",
            "PATSEY You will not be forgotten. Solomon moves out of the way, and enters the brick house. Patsey exits the house, Patsey running her fingers over the door handles. Solomon moves in close, and stares out for the drowned Patsey as she passes the house. As she passes, Solomon sees a boy running out to the field. He too is lost in the water of the mouth of the river; a slave who has passed the point of no return. As he boards a horse he spots Solomon, he too passes the boy. He hands him a rope, pulls him in close. \n",
            "INT. MASTER EPPS'S PLANTATION/FIELD - NIGHT \n",
            "Solomon is now in the middle of a field with several young children. He passes a young woman who is giving a talk. She stops and looks at Solomon. She is NOT A WOMAN \n",
            "(CONTINUED) \n",
            "36 CONTINUED: 36 \n",
            "SOLOMON ...I am giving a\n",
            "====================\n",
            "\n",
            "PATSEY All I ask: end my life. Take my body to the margin of the swamp-- Solomon looks at Patsey as though she were insane.\n",
            "SOLOMON No. \n",
            "PATSEY Take me by the throat. Hold me low in the water until I's still 'n without life. Bury me in a lonely place of dyin'. \n",
            "SOLOMON Patsey... \n",
            "(CONTINUED) \n",
            "105 CONTINUED: (2) 105 \n",
            "PATSEY They're all asleep. \n",
            "SOLOMON Not true, Patsey. Not true at all. \n",
            "PATSEY They are asleep. Solomon heads off. \n",
            "EXT. FORD PLANATION - DAY \n",
            "-END OF JANUARY, 1842- \n",
            "-AUGUST, 1842- \n",
            "And now for the trouble. The slaves are awake in the FORD PLANATION. They are preparing to depart for Lake Champlain. Solomon is among them, preparing for his trip as far as the timber store. \n",
            "INT. FORD PLANATION/BURY - LATER \n",
            "Solomon is preparing to depart for Lake Champlain. He is met by a large GROUP OF HANDS who surround him. They are VERY PHEAP, with the least amount of empathy. \n",
            "They ALL PASS BY A LITTLE AS WELL AS THE PLANATION SHOULD HAVE PHOOTED, but they are too busy trying to persuade Solomon to come along. \n",
            "INT. FORD PLANATION/DISTANT PLANTATION\n",
            "====================\n",
            "\n",
            "PATSEY All I ask: end my life. Take my body to the margin of the swamp-- Solomon looks at Patsey as though she were insane.\n",
            "SOLOMON No. \n",
            "PATSEY Take me by the throat. Hold me low in the water until I's still 'n without life. Bury me in a lonely place of dyin'. \n",
            "SOLOMON If I were to do so, I would whip my own head about, like a piece of flesh. \n",
            "(CONTINUED) \n",
            "105 CONTINUED: (2) 105 \n",
            "SOLOMON And strike the whip between the ribs? \n",
            "SOLOMON I will. It's stiffer now, Patsey-- \n",
            "PATSEY What's the whip stiffer? \n",
            "SOLOMON It's stiffer now, Patsey. It's because I have no lashings. \n",
            "PATSEY Who's stiffer? \n",
            "SOLOMON I was called to kill. C'mon, now. Catch me. Pull yourself together. Thoroughly whip yourself. Patsey turns to Solomon. \n",
            "PATSEY (CONT'D) Catch her. Catch him. \n",
            "SOLOMON I will. Patsey turns her head to the side. \n",
            "SOLOMON What's the whip stiffer? \n",
            "PATSEY I will teach you. Help me to my feet. \n",
            "(CONTINUED) \n",
            "105 CONTINUED: (3) 105 \n",
            "SOLOMON I will.\n",
            "====================\n",
            "\n",
            "PATSEY All I ask: end my life. Take my body to the margin of the swamp-- Solomon looks at Patsey as though she were insane.\n",
            "SOLOMON No. \n",
            "PATSEY Take me by the throat. Hold me low in the water until I's still 'n without life. Bury me in a lonely place of dyin'. \n",
            "SOLOMON Patsey... \n",
            "(CONTINUED) \n",
            "2A. \n",
            "3 CONTINUED: 3 \n",
            "SOLOMON Patsey! Patsey! Solomon looks at him in a rage, but he doesn't care. As if to remind himself of the sullen fate he left behind, Solomon turns and returns to his work. \n",
            "4 INT. NORTHUP HOUSE - NIGHT \n",
            "5 We are in the Northup House. Solomon is in the rear-seat of the car, and the only light is the occasional flash of a shot of Phebe. Two servants wait in the middle of the plattel, ALMOST LITTLE WIDDING MORE THAN A HOUSE. It is Anne who makes the sound of the whip being pulled. \n",
            "6 INT. NORTHUP HOUSE - EVENING \n",
            "It's Sunday. Patsey and Solomon are out playing. They are, comparatively speaking, the more physically fit of the couple. They have the property to themselves. The slaves are more than adequate. The account given of their sex life is one of pleasure and depression. One slave, Clemens, explains himself to the other. \n",
            "CLEMENS They're\n",
            "====================\n",
            "\n",
            "PATSEY All I ask: end my life. Take my body to the margin of the swamp-- Solomon looks at Patsey as though she were insane.\n",
            "SOLOMON No. \n",
            "PATSEY Take me by the throat. Hold me low in the water until I's still 'n without life. Bury me in a lonely place of dyin'. \n",
            "SOLOMON Patsey, I'll take that shirt off. \n",
            "(CONTINUED) \n",
            "105 CONTINUED: (5) 105 \n",
            "SOLOMON Yes, sir. \n",
            "PATSEY Master Ford will procure a keg of nails from Patsey. They will be tied to the ankle of the bed. \n",
            "SOLOMON And you will transport them to the Mistress, where they will be tied with the nails. \n",
            "FORD Patsey, I'll take that shirt off. \n",
            "(CONTINUED) \n",
            "105 CONTINUED: (6) 105 \n",
            "SOLOMON Yes, Master Ford will endeavor to procure a keg of nails. \n",
            "FORD Patsey, this is very good, sir. \n",
            "SOLOMON I will commence my journey in a few days, sir. \n",
            "FORD Patsey, I will commence my journey with you. \n",
            "EXT. FORD PLANTATION/PORT - DAY \n",
            "With the help of AID and EXT. FORD PLANTATION/PORT - DAY \n",
            "The slaves are now in their new location. They stand at a point just beyond the water's edge, hands\n",
            "====================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zjjEN2Tafhl2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "For bulk generation, you can generate a large amount of text to a file and sort out the samples locally on your computer. The next cell will generate a generated text file with a unique timestamp and then download it.\n",
        "\n",
        "You can rerun the cell as many times as you want for even more generated texts!"
      ]
    },
    {
      "metadata": {
        "id": "Fa6p6arifSL0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gen_file = 'gpt2_gentext_{:%Y%m%d_%H%M%S}.txt'.format(datetime.utcnow())\n",
        "\n",
        "gpt2.generate_to_file(sess,\n",
        "                      destination_path=gen_file,\n",
        "                      length=500,\n",
        "                      temperature=0.7,\n",
        "                      nsamples=100,\n",
        "                      batch_size=20\n",
        "                      )\n",
        "\n",
        "files.download(gen_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ig-KVgkCDCKD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Etcetera\n",
        "\n",
        "If the notebook has errors (e.g. GPU Sync Fail or out-of-memory/OOM), force-kill the Colaboratory virtual machine and restart it with the command below:"
      ]
    },
    {
      "metadata": {
        "id": "rIHiVP53FnsX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wmTXWNUygS5E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# LICENSE\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 Max Woolf\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    }
  ]
}